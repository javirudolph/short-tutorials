---
title: "Beta Binomial Example"
output: html_document
---


```{r}
knitr::opts_chunk$set(eval= TRUE)
```


```{r message=FALSE, warning=FALSE}
library("extraDistr")
library("MASS")

library(rjags)
library(parallel)
library(dclone)
library(abind)
library(R2WinBUGS)
library(RCurl)
```

This is an example of binomial survival, where we follow the fate of $n$ individuals during a single time interval. We let $X = \text{number of survivors}$ be binomially distributed:
$$f(x|p) = \binom{n}{x}p^{x}(1-p)^{n-x}$$
In the absence of previous information we may postulate that the prior distribution of $p$ is uniform:
$$g(p)=1, \quad 0\leq p \leq 1$$
Suppose $n=20$ and $x=5$. The posterior distribution is
$$h(p|x)\propto f(x|p)g(p) = \binom{n}{x}p^x(1-p)^{n-x}$$
This posterior distribution representes the belief of a Bayesian who was initially indiferent to the value of $p$ after observing 5 survivals in 20 trials. The uniform prior expresses an indiference about the possible values of $p$, which is modified after the experiment. 

# Quantifying prior beliefs using a Beta distribution

If instead, we use a Beta distribution for $p$, we get:
$$g(p) \propto p^{a-1}(1-p)^{b-1}$$
and the posterior distribution of $p$ given $x$ is:
$$h(p|x)\propto f(x|p)g(p) = \binom{n}{x}p^{x}(1-p)^{n-x}p^{a-1}(1-p)^{b-1}\propto p^{a+x-1}(1-p)^{n+b-x-1}$$
which is a new beta distribution with parameters $a' = a+x$ and $b' = n+b-x$. 

Note that $\mu_{prior} = \frac{a}{a+b}$ whereas
$$\mu_{post} = \frac{a'}{a'+b'} = \frac{a+x}{a+b+n} = \frac{a+x}{a+b+n} \color{blue}{\left( \frac{a}{a+b} \right)} + \frac{n}{a+b+n}\color{blue}{\bar{x}}$$
where $\bar{x} = x/n$ is the sample mean.  
The posterior mean is a \textcolor{blue}{weighted} average of the prior mean and the sample mean! As $n$ grows large, $x/n$ approaches true $p_0$ value and $\mu_post$ approaches $p_0$ (and variance goes to 0).

## Example using R
We simulate a vector of data, where survival is binomially distributed, and the probability of survival is randomly sampled from a beta distribution.
```{r}
n <- 30
a <- 2 #true value of a
b <- 5 # true value of b
ntrials <- 20

# We randomly sample values of p from a beta distribution using the true parameters a and b
P <- rbeta(n=n, shape1=a, shape2=b)
P
# And for each (n=30), we calculate the survival for the individuals (number of successes out of 20 trials) based on P.
YgP <- rbinom(n=n, size=ntrials, prob=P)
YgP
```

Now, using the data we created, we can estimate the parameters a and b, based on the data. We should get values close to the true values of a and b, which we used to simulate the data in the first place. The following function returns the sum of the negative log likelihood for a beta binomial distribution, which we will use for getting the maximum likelihood estimators for a and b.

```{r}
bbnegllike <- function(guess, Yvec, ntrials=20){
  
  pos.guess <- exp(guess)
  a <- pos.guess[1]
  b <- pos.guess[2]
  
  bbnegll.vec <- dbbinom(x=Yvec, size=ntrials, alpha=a, beta=b, log=TRUE)
  
  return(-sum(bbnegll.vec))
  
}
```

```{r}

# Using the function described above and the sample data from Y given P
myguess <- log(c(1.5,4)) # My guess for the parameters a and b
opt.samp <- optim(par=myguess, fn=bbnegllike, method="Nelder-Mead",
                  Yvec=YgP, ntrials=ntrials)
a.hat <- exp(opt.samp$par[1])
b.hat <- exp(opt.samp$par[2])

# These are the estimates for the parameters a and b, based on the data we provided
# We can compare them to the true values we used for simulating the data
knitr::kable(data.frame(a.hat = a.hat, a = a, b.hat = b.hat, b=b))
```


# Using MCMC:  first a Bayesian approach.  
Then Data Cloning which includes the Bayesian results as a special case
The first thing to do is to put together a function with the posterior calculations that needs to be sent to JAGS.  This function is written in  JAGS language 

```{r}
beta.binom <- function(){
  
  # needs:
  # a vector of counts (Y) 
  # the length of this vector (len)
  # the number of binomial trials (ntrials)
  
  # priors for a and b
  log.a~dnorm(mu.prior,precis.prior)
  a <- exp(log.a)
  
  log.b~dnorm(mu.prior,precis.prior)
  b <- exp(log.b)
  
  # Random effects model  
  for(i in 1:len){
    Prand[i] ~ dbeta(a,b)
  }

  # Likelihood (conditioned on the random effect)
  for(i in 1:len){
    Y[i] ~ dbinom(Prand[i],ntrials)
  }  

}

```

So now, using the same data that we simulated before, we use a Bayesian approach to estimate the parameters of interest. 

```{r}

out.parms <- c("a","b", "Prand")
jags.list <- list(Y=YgP, len=n, ntrials=ntrials, mu.prior=0, precis.prior=0.00001)

bayesian.fit <- jags.fit(jags.list, params=out.parms, model=beta.binom, 
                         n.chains=3, n.adapt=3000, n.update=1000, n.iter=30000, thin=10)

```

The output of the object `bayesian.fit` is a list of 3 chains. This is estimating parameters a, b, and the random P that gets drawn from the beta distribution.
```{r fig.height=12}
plot(bayesian.fit)
```

```{r}
summary(bayesian.fit)

```

# Implementing "Data Cloning", use uniform priors
The key difference is entering the replicated (cloned) data. How do you do that? well, you simply copy the data 'K' times. If K=1, the results correspond to a Bayesian analysis, and as K grows large, the resulting posterior converges to a MVNorm whose mean is equal to the MLEs and variance is equal to (1/K)*Inverse(Fisher's info)

```{r}
DC.beta.binom <- function(){
  
  # needs:
  # a vector of counts (Y) 
  # the length of this vector (len)
  # the number of binomial trials (ntrials)
  
  # priors for a and b
  a~dunif(low.prior,high.prior)
  b~dunif(low.prior,high.prior)

 
    # Random effects model  
  for(i in 1:len){
    for(k in 1:K){    
      Prand[i,k] ~ dbeta(a,b)
    }
  }
   
   # Likelihood (conditioned on the random effect)
  for(i in 1:len){
    for(k in 1:K){
      Y[i,k] ~ dbin(Prand[i,k],ntrials)
    }  
  }
    
}
```


Simulating data again, although I don't think this is necessary since we've already simulated this data in the first section of this document. 
```{r}
##### Simulating data:
n <- 30
a <- 2
b <- 5
ntrials <- 20

P <- rbeta(n=n, shape1=a, shape2=b)
YgP <- rbinom(n=n, size=ntrials, prob=P)
```


## Bayesian fit, K = 1 
```{r}
K <- 1
prior.low <- 0
prior.high <- 100
out.parms <- c("a", "b")
Ymat <- matrix(rep(YgP, K), nrow=n,ncol=K, byrow=FALSE)
dc.list <- list(Y=Ymat, len=n, ntrials=ntrials, low.prior=prior.low, high.prior=prior.high, K=K)
bayesian.fit <- jags.fit(dc.list, params=out.parms, model=DC.beta.binom, 
                         n.chains=3, n.adapt=3000, n.update=1000, n.iter=30000, thin=10)
summary(bayesian.fit)[[1]]

plot(bayesian.fit)

Cred.Ints <- apply(bayesian.fit[[1]], 2,FUN=function(x){quantile(x,probs=c(0.025,0.5,0.975))})
```


### STRAIGHT UP NUMERICAL OPTIMIZATION MLE ESTIMATION 

```{r}
myguess <- log(c(1.5,4))
opt.samp <- optim(par=myguess, fn=bbnegllike, method="Nelder-Mead", Yvec=YgP, ntrials=ntrials)
a.hat <- exp(opt.samp$par[1])
b.hat <- exp(opt.samp$par[2])
print(a.hat)
print(b.hat)
```


#------------------------------------------------------
## Data cloning with K=32
```{r}

K <- 32
prior.low <- 0
prior.high <- 100
out.parms <- c("a", "b")
Ymat <- matrix(rep(YgP, K), nrow=n,ncol=K, byrow=FALSE)
dc.list <- list(Y=Ymat, len=n, ntrials=ntrials, low.prior=prior.low, high.prior=prior.high, K=K)
ml.fit <- jags.fit(dc.list, params=out.parms, model=DC.beta.binom, 
                         n.chains=3, n.adapt=4000, n.update=1000, n.iter=50000, thin=10)
summary(ml.fit)[[1]]

plot(ml.fit)
```

########  Wald Confidence Intervals ########
```{r}

kth.post <- ml.fit[[1]]
mles <- apply(kth.post, 2,mean)
fish.inv <- K*var(kth.post)
alpha <- 0.05
z.alpha.half <- qnorm(p=(1-alpha/2))
a.std.error <- z.alpha.half*sqrt(fish.inv[1,1])
b.std.error <- z.alpha.half*sqrt(fish.inv[2,2])
a.cis <- c(mles[1]-a.std.error,mles[1], mles[1]+a.std.error)
b.cis <- c(mles[2]-b.std.error,mles[2], mles[2]+b.std.error)

CIs.mat <- rbind(a.cis,b.cis);
colnames(CIs.mat) <- c("LCL", "MLE", "UCL")
rownames(CIs.mat) <- c("a","b")
print(CIs.mat) # Compare to 
print(Cred.Ints)
```


#######################  ASSIDE: TO UNDERSTAND THE REST, I NEED MONTE CARLO INTEGRATION######
########  Numerical Integration example #########

#####  \int h(x)dx  = \int h(x) (f(x)/f(x)) dx
#####               = \int (h(x)/f(x))*f(x) dx
#####              == E[(h(x)/f(x))] with respect of the distribution f(x)

$$\int h(x)dx  = \int h(x) \frac{f(x)}{f(x)} dx$$
$$               = \int \frac{h(x)}{f(x)}\times f(x) dx $$
$$            == E\left[\frac{h(x)}{f(x)} \right] \text{with respect of the distribution} f(x)$$

# Example of Monte Carlo integration
# Say I need to compute Var(Y), where Y~exp(lambda=3).
# Var(Y) = I = int_{0}^{Infty} (y-(1/lambda))^2 *lambda*exp(-y*lambda) dy

```{r}
# First, set the value of lambda
lam <- 3
nsamps <- 1000000


# Now sample from another distribution with support in (0,Infty), say an exp(theta=4)
theta <- 4
y.samps <- rexp(n=nsamps, rate=theta)


mc.integrand <- (((y.samps-(1/lam))^2)*lam*exp(-y.samps*lam))/(theta*exp(-y.samps*theta))

real.var <- 1/(lam^2)
real.var
mean(mc.integrand)# LLN
```



#---------------------------------------------------------------


Now, obtaining MCMC samples from the distribution of the hidden process given the observations.  If $f(y|x)$ is the likelihood and $g(x;theta)$ is the hidden process, then this posterior distribution is $h(x|y;theta) \propto f(y|x)g(x;theta)$ which we can sample from using a simple MCMC run.

```{r}

Kalman.beta.binom <- function(){
  
  # needs:
  # a vector of counts (Y) 
  # the length of this vector (len)
  # the number of binomial trials (ntrials)
  # The values of a and b = the MLES  

  # Random effects model  
  for(i in 1:len){
      Prand[i] ~ dbeta(a,b)
  }
  
  # Likelihood (conditioned on the random effect)
  for(i in 1:len){
      Y[i] ~ dbin(Prand[i],ntrials)
  }
  
}

```

```{r}

out.parms <- c("Prand")
kalman.list <- list(Y=YgP, len=n, ntrials=ntrials, a=mles[1], b=mles[2])
kalman.samps <- jags.fit(kalman.list, params=out.parms, model=Kalman.beta.binom, 
                   n.chains=3, n.adapt=4000, n.update=1000, n.iter=50000, thin=10)
summary(kalman.samps)
plot(kalman.samps)
```



```{r}

bo <- 1.5
b1 <- 2
sigsq <- 0.3
sig <- sqrt(sigsq)
N <- 300
x  <- runif(n=N, min=-3,max=3)
mu.x <- bo+b1*x
y  <- rnorm(n=N,mean=mu.x,sd=sqrt(sigsq))


my.lm <- function(guess,y,x){
  
  bo <- guess[1]
  b1 <- guess[2]
  sig<- exp(guess[3])
  
  mu.x <- bo+b1*x
  nllike <- -sum(dnorm(x=y,mean=mu.x,sd=sig, log=TRUE))
  return(nllike)  
  
}

lm.fit <- optim(par=c(1.5,3,log(sqrt(0.2))), fn=my.lm, method="BFGS", y=y,x=x)

bo.h <- lm.fit$par[1]
b1.h <- lm.fit$par[2]
sig.h <- exp(lm.fit$par[3])
sig.h^2

my.loglik <- -lm.fit$value

rlm.fit <- lm(y~1+x)
anova(rlm.fit)
print(logLik(rlm.fit))



binom.Ho <- function(m1,p1){
  
  n.guilds <- length(m1)
  N.vec <- m1+p1
  
  llike <- dbinom()
  
  
}
```

